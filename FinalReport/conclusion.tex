Our results show that FPGAs are a viable option for speeding up the training phase for CNNs. However, we also note that these implementations are constrained by the resources available, and thus, FPGA implementations are only useful if the network can be partitioned. In particular, training a small to medium sized networks on a single CPU or GPU is likely a better option than an FPGA implementation. However, as the input data and the network depth scales up, the benefits of CPU implementations fail to deliver speed-up. Particularly, at the scale of data centers like Microsoft \cite{project-catapult}, an FPGA implementation offers the necessary speed-up while still being more power efficient than an equivalent GPU offering.

Furthermore, our loss convergence results illustrate the need to explore limited precision training more deeply. Currently, neural network designers add controlled noise to the data or model (via dropout) in order to increase the model robustness. We would like to explore using quantization error to introduce this noise. The properties of quantization error can be controlled and bounded via varying rounding schemes. Moreover, the benefits of quantization and limited precision are two-fold -- first, controlled noise can make the model more robust if inserted intelligently; second, the reduced bit length leads to faster hardware and more resources on-chip to create parallel execution units.

In summary, this project illustrates a promising direction for further exploration. In the future, we would like to implement partitioning networks across several FPGAs to increase the clock frequency and number of parallel execution units, as well as understand the effects of quantization and limited precision on the robustness of CNNs. We conclude that FPGAs are a suitable candidate for speed-up when the networks and datasets are sufficiently large, and when aggregate power consumption is costly.