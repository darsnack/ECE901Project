Convolutional neural networks are optimized for classification of structured data, such as images, and map input data into labels through successive representation layers.  Each layer abstracts information about the layer previous through a nonlinear activation function.  
To find a classifier that abstracts patterns in input data, we would like to minimize the empirical risk on the training data, given $m$ training example-label pairs $\{(x_i,y_i)\}_{i=1}^m$.
\begin{equation}
	\min_{f\in F}\sum_{i=1}^n \mathcal{L}(f(x_i);y_i)
	\label{eq:erm}
\end{equation}
where $\mathcal{L}$ is some loss function assigns a value related to the discrepancy between the label generated by the model $f$ and the true label.  SGD is used to solve the ERM, but its serial nature motivates the investigation of methods that achieve speedup.
