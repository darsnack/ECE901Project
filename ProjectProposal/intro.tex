Machine learning systems tackle problems ranging from content filtering and recommender systems to object recognition and text classification.  In recent years, advances in deep learning have led to finding better classifiers for these problems.  In deep learning, feature extraction is performed automatically by using many layers of neural networks; each layer involves passing inputs of the previous layer through a nonlinear activation function.  Compositions of successive layers can thus correspond to learning nonlinear decision boundaries, which has contributed to successes in image classification  and speech recognition.  Training of deep neural networks is performed using the stochastic gradient descent (SGD) algorithm, but the inherently serial nature of SGD has led to exploration of potential speedups through parallelized hardware.  In this project, we seek to investigate how the computational cost per iteration of training a deep neural network depends on the hardware architecture being used. 
